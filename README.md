# ğŸ” Fake News Detector - Free Version

**100% kostenlos** mit GitHub Actions â€“ kein Server nÃ¶tig!

## So funktioniert's

1. GitHub Actions fÃ¼hrt den Crawler alle 6 Stunden aus
2. Ergebnisse werden als JSON/CSV im Repo gespeichert
3. Du kannst die Ergebnisse im Dashboard ansehen

## Setup (5 Minuten)

### 1. Fork dieses Repo

Klicke auf "Fork" oben rechts.

### 2. Aktiviere GitHub Actions

Gehe zu `Settings` â†’ `Actions` â†’ `General` â†’ Enable "Allow all actions"

### 3. Optional: API Key hinzufÃ¼gen

FÃ¼r bessere Fact-Check-Ergebnisse:
1. Hole einen [Google Fact Check API Key](https://developers.google.com/fact-check/tools/api/v1alpha1/factchecktools)
2. Gehe zu `Settings` â†’ `Secrets and variables` â†’ `Actions`
3. Klicke "New repository secret"
4. Name: `GOOGLE_FACTCHECK_API_KEY`, Value: dein Key

### 4. Manuell starten (optional)

Gehe zu `Actions` â†’ `Fake News Crawler` â†’ `Run workflow`

## Ergebnisse ansehen

### Option A: Im Repo
- `results/latest.json` â€“ Aktuelle Ergebnisse
- `results/results_YYYYMMDD_HHMM.json` â€“ Archiv

### Option B: Dashboard
1. Aktiviere GitHub Pages: `Settings` â†’ `Pages` â†’ Source: `main` / `root`
2. Ã–ffne `https://<dein-username>.github.io/<repo-name>/`
3. Lade `results/latest.json`

### Option C: Download
Gehe zu `Actions` â†’ Klicke auf einen Run â†’ Download "results" Artifact

## Kosten

**$0** â€“ GitHub Actions Free Tier beinhaltet:
- 2000 Minuten/Monat
- Dieser Crawler braucht ~2-3 Min pro Run
- Bei 4 Runs/Tag = ~360 Min/Monat âœ…

## Anpassen

### Crawl-Frequenz Ã¤ndern

In `.github/workflows/crawl.yml`:
```yaml
schedule:
  - cron: '0 */6 * * *'  # Alle 6 Stunden
  # Oder:
  - cron: '0 8,20 * * *'  # 2x tÃ¤glich (8:00 und 20:00 UTC)
  - cron: '0 12 * * *'    # 1x tÃ¤glich (12:00 UTC)
```

### Andere Sprachen

In `crawler_simple.py`:
```python
GDELT_LANGUAGES = ["german", "english", "french", "spanish"]
```

### Mehr/Weniger Artikel

```python
MAX_ARTICLES = 50  # Standard: 30
```

## Struktur

```
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ crawl.yml      # GitHub Action
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ latest.json        # Aktuelle Ergebnisse
â”‚   â””â”€â”€ results_*.json     # Archiv
â”œâ”€â”€ crawler_simple.py      # Hauptscript
â”œâ”€â”€ index.html             # Dashboard
â””â”€â”€ README.md
```

## Limitierungen

- Keine Echtzeit-Analyse (nur alle paar Stunden)
- Keine persistente Datenbank (nur JSON-Dateien)
- Kein Web-API (nur statische Dateien)

FÃ¼r eine Always-On-LÃ¶sung mit API: Siehe die `fake-news-service` Version (~$5/Monat).

## FAQ

**Q: Warum alle 6 Stunden und nicht Ã¶fter?**
A: Um im Free Tier zu bleiben. Du kannst es auf stÃ¼ndlich Ã¤ndern, aber dann ~720 Min/Monat.

**Q: Kann ich RSS-Feeds hinzufÃ¼gen?**
A: Ja! Erweitere `crawler_simple.py` mit der RSS-Logik aus der Service-Version.

**Q: Wo sind meine alten Ergebnisse?**
A: Im `results/` Ordner oder unter Actions â†’ Artifacts (30 Tage).
