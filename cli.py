#!/usr/bin/env python3
"""
Fake News Detection - Command Line Interface
=============================================
Einfaches CLI f√ºr die MVP Pipeline.

Nutzung:
    python cli.py https://example.com/article
    python cli.py --file urls.txt
    python cli.py --file urls.txt --output results.csv
"""

import argparse
import sys
from main import run_pipeline, process_url, extract_article, extract_claims

def main():
    parser = argparse.ArgumentParser(
        description="üîç Fake News Detection Pipeline - MVP",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Beispiele:
  python cli.py https://example.com/news-article
  python cli.py --file urls.txt --output meine_results.csv
  python cli.py --analyze https://example.com/article
        """
    )
    
    parser.add_argument(
        'url',
        nargs='?',
        help='URL eines Artikels zum Pr√ºfen'
    )
    
    parser.add_argument(
        '--file', '-f',
        help='Textdatei mit URLs (eine pro Zeile)'
    )
    
    parser.add_argument(
        '--output', '-o',
        default='fake_news_results.csv',
        help='Output-Dateiname (default: fake_news_results.csv)'
    )
    
    parser.add_argument(
        '--analyze', '-a',
        action='store_true',
        help='Zeige detaillierte Analyse (nur Extraktion, kein Fact-Check)'
    )
    
    args = parser.parse_args()
    
    # Sammle URLs
    urls = []
    
    if args.url:
        urls.append(args.url)
    
    if args.file:
        try:
            with open(args.file, 'r') as f:
                file_urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                urls.extend(file_urls)
        except FileNotFoundError:
            print(f"‚ùå Datei nicht gefunden: {args.file}")
            sys.exit(1)
    
    if not urls:
        parser.print_help()
        print("\n‚ö†Ô∏è  Bitte mindestens eine URL angeben!")
        sys.exit(1)
    
    # Nur Analyse-Modus
    if args.analyze:
        for url in urls:
            print(f"\n{'='*60}")
            print(f"üì∞ Analysiere: {url}")
            print(f"{'='*60}")
            
            article = extract_article(url)
            if article:
                print(f"\nüìÑ Titel: {article.title}")
                print(f"üåê Domain: {article.source_domain}")
                print(f"üìù Text-L√§nge: {len(article.text)} Zeichen")
                print(f"#Ô∏è‚É£  Hash: {article.content_hash}")
                
                claims = extract_claims(article)
                print(f"\nüéØ {len(claims)} Claims gefunden:")
                for i, claim in enumerate(claims, 1):
                    print(f"\n   [{i}] (Confidence: {claim.confidence:.0%})")
                    print(f"       \"{claim.text}\"")
            else:
                print("‚ùå Extraktion fehlgeschlagen")
        return
    
    # Vollst√§ndige Pipeline
    results = run_pipeline(urls, args.output)
    
    if not results:
        print("\n‚ö†Ô∏è  Keine Ergebnisse generiert")
        sys.exit(1)

if __name__ == "__main__":
    main()
